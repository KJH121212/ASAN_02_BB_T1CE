{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d596ae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 100 BB–T1CE pairs.\n",
      "[CHECK] BB shape: torch.Size([178, 1, 240, 240])\n",
      "[CHECK] T1CE shape: torch.Size([178, 1, 240, 240])\n"
     ]
    }
   ],
   "source": [
    "loader = get_bb2t1ce_dataloader(BB_DIR, T1CE_DIR, batch_size=1)\n",
    "batch = next(iter(loader))\n",
    "bb, t1ce = batch[\"bb\"], batch[\"t1ce\"]\n",
    "\n",
    "print(f\"[CHECK] BB shape: {bb.shape}\")\n",
    "print(f\"[CHECK] T1CE shape: {t1ce.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2f3a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 100 BB–T1CE pairs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded batch: BB torch.Size([1, 1, 178, 240, 240]), T1CE torch.Size([1, 1, 178, 240, 240])\n",
      "✅ Flattened: BB torch.Size([178, 1, 240, 240]), T1CE torch.Size([178, 1, 240, 240])\n"
     ]
    }
   ],
   "source": [
    "# ✅ Dataset test\n",
    "loader = get_bb2t1ce_dataloader(BB_DIR, T1CE_DIR, batch_size=1, as_2d=False)\n",
    "batch = next(iter(loader))\n",
    "bb, t1ce = batch[\"bb\"], batch[\"t1ce\"]\n",
    "print(f\"✅ Loaded batch: BB {bb.shape}, T1CE {t1ce.shape}\")\n",
    "\n",
    "# ✅ 3D → 2D flatten\n",
    "def collapse_z_to_batch(tensor):\n",
    "    \"\"\"(B, C, Z, H, W) → (B*Z, C, H, W)\"\"\"\n",
    "    if tensor.ndim == 5:\n",
    "        B, C, Z, H, W = tensor.shape\n",
    "        tensor = tensor.permute(0, 2, 1, 3, 4).reshape(B * Z, C, H, W)\n",
    "    return tensor\n",
    "\n",
    "bb = collapse_z_to_batch(bb)\n",
    "t1ce = collapse_z_to_batch(t1ce)\n",
    "print(f\"✅ Flattened: BB {bb.shape}, T1CE {t1ce.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc00d96",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bb2t1ce_transform\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ✅ Transform 적용 (2D 전용)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m bb_t, t1ce_t \u001b[38;5;241m=\u001b[39m \u001b[43mbb2t1ce_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1ce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Transform applied: BB \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbb_t\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, T1CE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1ce_t\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/nas100/forGPU2/Kimjihoo/ASAN_02_BB_T1CE/datasets/transforms.py:77\u001b[0m, in \u001b[0;36mbb2t1ce_transform\u001b[0;34m(bb_tensor, t1ce_tensor, n_slices, crop_size, random_flip_prob, center_crop_prob)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbb2t1ce_transform\u001b[39m(bb_tensor, t1ce_tensor,\n\u001b[1;32m     72\u001b[0m                       n_slices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     73\u001b[0m                       crop_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m     74\u001b[0m                       random_flip_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     75\u001b[0m                       center_crop_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"5D 입력 대응 (B,C,Z,H,W)\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     bb_tensor, t1ce_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbb_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1ce_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_slices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m center_crop_prob:\n\u001b[1;32m     80\u001b[0m         bb_tensor \u001b[38;5;241m=\u001b[39m center_crop(bb_tensor, crop_size)\n",
      "File \u001b[0;32m/workspace/nas100/forGPU2/Kimjihoo/ASAN_02_BB_T1CE/datasets/transforms.py:26\u001b[0m, in \u001b[0;36mrandom_slice\u001b[0;34m(bb_tensor, t1ce_tensor, n_slices)\u001b[0m\n\u001b[1;32m     24\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, z \u001b[38;5;241m-\u001b[39m n_slices)\n\u001b[1;32m     25\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m start_idx \u001b[38;5;241m+\u001b[39m n_slices\n\u001b[0;32m---> 26\u001b[0m bb_crop \u001b[38;5;241m=\u001b[39m \u001b[43mbb_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     27\u001b[0m t1ce_crop \u001b[38;5;241m=\u001b[39m t1ce_tensor[:, :, start_idx:end_idx, :, :]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bb_crop, t1ce_crop\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/monai/data/meta_tensor.py:283\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 283\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:1654\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1654\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1656\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
     ]
    }
   ],
   "source": [
    "from datasets.transforms import bb2t1ce_transform\n",
    "\n",
    "# ✅ Transform 적용 (2D 전용)\n",
    "bb_t, t1ce_t = bb2t1ce_transform(bb, t1ce)\n",
    "print(f\"✅ Transform applied: BB {bb_t.shape}, T1CE {t1ce_t.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea8af7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 경로: /workspace/nas100/forGPU2/Kimjihoo/ASAN_02_BB_T1CE\n",
      "✅ Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.spatial.dictionary Orientationd.__init__:labels: Current default value of argument `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` was changed in version None from `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` to `labels=None`. Default value changed to None meaning that the transform now uses the 'space' of a meta-tensor, if applicable, to determine appropriate axis labels.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/transform.py\", line 150, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/spatial/dictionary.py\", line 868, in __call__\n    d[key] = self.resizer(\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/spatial/array.py\", line 858, in __call__\n    raise ValueError(\nValueError: len(spatial_size) must be greater or equal to img spatial dimensions, got spatial_size=2 img=3.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/monai/data/dataset.py\", line 109, in __getitem__\n    return self._transform(index)\n  File \"/opt/conda/lib/python3.10/site-packages/monai/data/dataset.py\", line 95, in _transform\n    return self.transform(data_i)\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/compose.py\", line 346, in __call__\n    result = execute_compose(\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/compose.py\", line 116, in execute_compose\n    data = apply_transform(\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/transform.py\", line 180, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.spatial.dictionary.Resized object at 0x7fa0e0e810f0>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 65\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# 4️⃣ Dataset / DataLoader\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     57\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m get_bb2t1ce_dataloader(\n\u001b[1;32m     58\u001b[0m     bb_dir\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbb_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     59\u001b[0m     t1ce_dir\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt1ce_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     as_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# 3D→2D 슬라이스 변환 (Diffusion 학습용)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m sample_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Sample batch loaded: BB \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbb\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, T1CE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt1ce\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# 5️⃣ 모델 정의\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Conditional UNet은 x_t, timestep t, 조건 BB를 입력으로 받음\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1506\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1504\u001b[0m worker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1541\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:769\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 769\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/transform.py\", line 150, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/spatial/dictionary.py\", line 868, in __call__\n    d[key] = self.resizer(\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/spatial/array.py\", line 858, in __call__\n    raise ValueError(\nValueError: len(spatial_size) must be greater or equal to img spatial dimensions, got spatial_size=2 img=3.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/monai/data/dataset.py\", line 109, in __getitem__\n    return self._transform(index)\n  File \"/opt/conda/lib/python3.10/site-packages/monai/data/dataset.py\", line 95, in _transform\n    return self.transform(data_i)\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/compose.py\", line 346, in __call__\n    result = execute_compose(\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/compose.py\", line 116, in execute_compose\n    data = apply_transform(\n  File \"/opt/conda/lib/python3.10/site-packages/monai/transforms/transform.py\", line 180, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.spatial.dictionary.Resized object at 0x7fa0e0e810f0>\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# test.py\n",
    "# ASAN_02_BB_T1CE - 전체 모듈 통합 테스트 (MONAI 기반)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ 프로젝트 루트 경로 설정\n",
    "# ------------------------------------------------------------\n",
    "ROOT_DIR = \"/workspace/nas100/forGPU2/Kimjihoo/ASAN_02_BB_T1CE\"\n",
    "os.chdir(ROOT_DIR)\n",
    "sys.path.append(ROOT_DIR)\n",
    "print(f\"현재 경로: {os.getcwd()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ 모듈 임포트\n",
    "# ------------------------------------------------------------\n",
    "from datasets.bb2t1ce_dataset import get_bb2t1ce_dataloader\n",
    "from models.diffusion_unet_custom import DiffusionUNetCustom as ConditionalUNet\n",
    "from runners.trainer import Trainer\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ 기본 설정\n",
    "# ------------------------------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {DEVICE}\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"data\": {\n",
    "        \"bb_dir\": \"./data/0_RAW_DATA/meta-bb\",\n",
    "        \"t1ce_dir\": \"./data/0_RAW_DATA/meta-t1ce\",\n",
    "        \"batch_size\": 1,\n",
    "        \"num_workers\": 2,\n",
    "    },\n",
    "    \"diffusion\": {\n",
    "        \"timesteps\": 10,               # 빠른 테스트용\n",
    "        \"beta_start\": 1e-4,\n",
    "        \"beta_end\": 0.02,\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\": 2,                 # 빠른 테스트\n",
    "        \"loss_type\": \"l2\",\n",
    "        \"ckpt_dir\": \"./checkpoints_test\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣ Dataset / DataLoader\n",
    "# ------------------------------------------------------------\n",
    "train_loader = get_bb2t1ce_dataloader(\n",
    "    bb_dir=CONFIG[\"data\"][\"bb_dir\"],\n",
    "    t1ce_dir=CONFIG[\"data\"][\"t1ce_dir\"],\n",
    "    batch_size=CONFIG[\"data\"][\"batch_size\"],\n",
    "    num_workers=CONFIG[\"data\"][\"num_workers\"],\n",
    "    as_2d=True  # 3D→2D 슬라이스 변환 (Diffusion 학습용)\n",
    ")\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"✅ Sample batch loaded: BB {sample_batch['bb'].shape}, T1CE {sample_batch['t1ce'].shape}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣ 모델 정의\n",
    "# ------------------------------------------------------------\n",
    "# Conditional UNet은 x_t, timestep t, 조건 BB를 입력으로 받음\n",
    "model = ConditionalUNet(\n",
    "    in_channels=1,\n",
    "    cond_channels=1,\n",
    "    base_channels=64,\n",
    "    channel_mults=[1, 2, 4],\n",
    "    num_res_blocks=2,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6️⃣ Trainer 초기화\n",
    "# ------------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    dataloader=train_loader,\n",
    "    config=CONFIG,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7️⃣ 학습 실행\n",
    "# ------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting BB→T1CE diffusion model training...\")\n",
    "    trainer.train()\n",
    "    print(\"✅ Training completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
